{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa938026-340e-4fa7-a418-3574b8bf0b52",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293a92d-8dff-433d-a2a0-5abcee9814bf",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1707.06347"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47036ea1-2b87-42a2-893b-70ed5d08c808",
   "metadata": {},
   "source": [
    "## 1. Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16660d6-a8cb-47b0-a307-5e79ce20a3fe",
   "metadata": {},
   "source": [
    "$$E(R(\\tau))_{\\tau \\sim P_\\theta(\\tau)} = \\sum_{\\tau}R(\\tau)P_\\theta(\\tau) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaaca79-f455-4e4b-889f-ddffeaab42e9",
   "metadata": {},
   "source": [
    "We want expected reward to be as large as possible. \\\n",
    "So we need to find its gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6c863e-6c1d-49c9-997d-629bf9220074",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align*}\n",
    "\\nabla E(R(\\tau))_{\\tau \\sim P_\\theta(\\tau)} &= \\nabla \\sum_{\\tau}R(\\tau)P_\\theta(\\tau)\\\\\n",
    "          &= \\sum_{\\tau}R(\\tau)\\nabla P_\\theta(\\tau)\\\\\n",
    "          &= \\sum_{\\tau}R(\\tau)\\nabla P_\\theta(\\tau)\\frac{P_\\theta(\\tau)}{P_\\theta(\\tau)}\\\\\n",
    "          &= \\sum_{\\tau}R(\\tau)P_\\theta(\\tau)\\frac{\\nabla P_\\theta(\\tau)}{P_\\theta(\\tau)}\\\\\n",
    "          &\\approx \\frac{1}{N}\\sum_{n=1}^{N}R(\\tau^n)\\frac{\\nabla P_\\theta(\\tau)}{P_\\theta(\\tau)} \\text{  Rough Average}\\\\\n",
    "          &= \\frac{1}{N}\\sum_{n=1}^{N}R(\\tau^n)\\nabla \\log P_\\theta(\\tau^n)\\\\\n",
    "          &= \\frac{1}{N}\\sum_{n=1}^{N}R(\\tau^n)\\nabla \\log \\prod_{t=1}^{T_n}P_\\theta(a_n^t \\mid s_n^t)\\\\\n",
    "          &= \\frac{1}{N}\\sum_{n=1}^{N}R(\\tau^n)\\sum_{t=1}^{T_n}\\nabla \\log P_\\theta(a_n^t \\mid s_n^t)\\\\\n",
    "          &= \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}R(\\tau^n)\\nabla \\log P_\\theta(a_n^t \\mid s_n^t)\\\\\n",
    "          &= \\nabla_\\theta J(\\theta)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6519892f-9357-41eb-b182-b95e17048278",
   "metadata": {},
   "source": [
    "$$Loss = -\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}R(\\tau^n)\\nabla \\log P_\\theta(a_n^t \\mid s_n^t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd1b0ea-0b45-4bc1-a198-339a46197935",
   "metadata": {},
   "source": [
    "**Basic Gradient Update**:\n",
    "$$\\text{1.Sample  } (\\tau)^i \\text{ from } \\pi_\\theta(a_t \\mid s_t)$$\n",
    "$$\\text{2.Compute gradient  }\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}R(\\tau^n)\\nabla \\log P_\\theta(a_n^t \\mid s_n^t)$$\n",
    "$$\\text{3.Take one step along gradient  } \\theta = \\theta + \\alpha \\nabla_\\theta J(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90093a-89c1-47e9-a6e9-a551a9280df9",
   "metadata": {},
   "source": [
    "**On Policy**: Use the same model to do data collection and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dda95c-e97e-426b-8661-dabf1e422014",
   "metadata": {},
   "source": [
    "**Problem 1**:\\\n",
    "$$ \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}R(\\tau^n)\\nabla \\log P_\\theta(a_n^t \\mid s_n^t)$$\n",
    "This means if the trajectory has **positive reward sum**, we would **raise all probabilities of actions** along this trajectory, which is obviously inefficient.\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4bba1-e16f-47ef-9a68-7f0ab420b708",
   "metadata": {},
   "source": [
    "So we should find the actual impact of an action.\\\n",
    "We'd like to find how current action affects future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62738e-8637-4ff6-8489-083c1033054b",
   "metadata": {},
   "source": [
    "Compute Future Reward:\n",
    "\n",
    "$$R(\\tau^n) \\rightarrow \\sum_{t' = t}^{T_n} \\gamma^{t' - t} r_{t'}^n = R_t^n  \\text{    (}\\gamma \\text{  is called discount factor)}$$\n",
    "$$\\text{replacement: }J(\\theta)=\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}R_t^n\\nabla \\log P_\\theta(a_n^t \\mid s_n^t)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a03402-25be-4557-b586-90a104661e56",
   "metadata": {},
   "source": [
    "$$ \\gamma<1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfefe6d-424d-43d8-99ee-0c060c0b033d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\gamma \\text{ decreases as t gets bigger(futher == less impact on future)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaebdf5-290a-47a1-8e28-b1b54ae48ae1",
   "metadata": {},
   "source": [
    "**Problem 2**:\\\n",
    "Let's say we have a good situation where all actions increases future reward, so we **increases all their probabilities**.This could be quite inefficient as probability sum remain one, meaning increment could only be subtle.(The same for all negative situation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e8b443-7481-4c09-aea6-5429f0d23269",
   "metadata": {},
   "source": [
    "So what we actually want is to significantly **increases probabilities of the actions that has \"big\" positive reward** and decrease those who has small positive reward.(Focus on **reletive reward**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a729c-7b9c-4d9b-a227-08639f8e378c",
   "metadata": {},
   "source": [
    "So we need to choose a proper **baseline** and reletive_reward = reward - baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723a1db9-57aa-49b1-bd96-1e63677f069a",
   "metadata": {},
   "source": [
    "$$ \\text{replacement:  }J(\\theta)=\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}(R_t^n-B(s_n^t))\\nabla \\log P_\\theta(a_n^t \\mid s_n^t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b78e7d-6522-4f40-ba72-a829c3092bbc",
   "metadata": {},
   "source": [
    "## 2. Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c6c0e0-e7b5-4a0e-9c8f-0421b0ef8185",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "To further optimize our algorithm, we introduce three fuctions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd631ff-dae0-42af-b83b-87d1edfcc647",
   "metadata": {},
   "source": [
    "**Action-Value Function**(Q-function):\\\n",
    "$$ Q_\\theta(s_t,a_t) = \\sum_{t'=t}^T E_{\\pi_\\theta}[(r(s_t',a_t')\\mid s_t,a_t)]$$ \n",
    "$$\\text{Expected total future reward by taking }a_t \\text{ in } s_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594185b3-70f7-4cfe-85e0-cc57fe2fa50d",
   "metadata": {},
   "source": [
    "**State-Value Function**(V-function):\\\n",
    "$$V_\\theta(s_t) = E_{a_t \\sim \\pi_\\theta(a_t\\mid s_t)}[Q^\\pi(s_t,a_t)]$$\n",
    "$$\\text{Expected total future reward in }s_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a638a62c-7434-4dd6-8ee3-380e6bb82423",
   "metadata": {},
   "source": [
    "**Advantage Function**:\\\n",
    "$$A_\\theta(s_t, a_t) = Q_\\theta(s_t,a_t)- V_\\theta(s_t)$$\n",
    "$$\\text{Gained advantage by taking  }a_t \\text{ in } s_t \\text{ compared with other actions}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f8f74-aae0-4aab-b81f-a5891deeb802",
   "metadata": {},
   "source": [
    "Noted that we need to fit two neural networks, one for Q-function and one for V-function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f275e9-fa31-438d-a5a7-e13cfe4d91f6",
   "metadata": {},
   "source": [
    "$$\\text{replacement:  } J(\\theta)=\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}A_\\theta(s_t, a_t)\\nabla \\log P_\\theta(a_n^t \\mid s_n^t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c35e1d-f9ec-48a7-89ee-922e8a24e5a2",
   "metadata": {},
   "source": [
    "Then what's the relation between Q and V?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ce47b-62d9-45b1-93d5-0c2bbf00cf97",
   "metadata": {},
   "source": [
    "We can make inference based on definition, then we have:\n",
    "$$ Q_\\theta(s_t,a_t) = r_t + \\gamma * V_\\theta(s_{t+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43202e09-9488-4bce-b11a-b004ee8af816",
   "metadata": {},
   "source": [
    "$$\\text{replacement:  }A_\\theta(s_t, a_t) = r_t + \\gamma * V_\\theta(s_{t+1})- V_\\theta(s_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f60419-b641-401d-9554-ffa5f7061846",
   "metadata": {},
   "source": [
    "**So we only need to fit one neural network which is V-fuction!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea466a84-3568-4d5e-ba3a-bd1e6e694666",
   "metadata": {},
   "source": [
    "Now we can do recursive sampling on V-function:\n",
    "$$\\text{recursive formula:  }V_\\theta(s_{t+1}) \\approx r_{t+1} + \\gamma* V_\\theta(s_t+2)$$\n",
    "\\begin{align*}\n",
    "&A^1_\\theta(s_t, a_t) = r_t + \\gamma * V_\\theta(s_{t+1})- V_\\theta(s_t)\\\\\n",
    "&A^2_\\theta(s_t, a_t) = r_t + \\gamma * r_{t+1} + \\gamma^2 * V_\\theta(s_{t+2}) - V_\\theta(s_t)\\\\\n",
    "&A^3_\\theta(s_t, a_t) = r_t + \\gamma * r_{t+1} + \\gamma^2 * V_\\theta(s_{t+2}) +\\gamma^3 * V_\\theta(s_{t+3})- V_\\theta(s_t)\\\\\n",
    "&A^T_\\theta(s_t, a_t) = r_t + \\gamma * r_{t+1} + \\gamma^2 * V_\\theta(s_{t+2}) +\\gamma^3 * V_\\theta(s_{t+3})+...+\\gamma^T * r_T- V_\\theta(s_t)\\\\\n",
    "\\\\\n",
    "&\\text{With more terms(bigger T), the bias goes down and the variance goes up}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f00c14a-49d0-43b7-91c9-f537c47ecb44",
   "metadata": {},
   "source": [
    "Simplified version:\n",
    "\\begin{align*}\n",
    "&\\sigma_t^V = r_t + \\gamma * V_\\theta(s_{t+1})- V_\\theta(s_t)\\\\\n",
    "&\\sigma_{t+1}^V = r_t + \\gamma * r_{t+1} + \\gamma^2 * V_\\theta(s_{t+2}) - V_\\theta(s_t+1)\\\\\n",
    "\\text{}\\\\\n",
    "\\text{replacement:  }&A^1_\\theta(s_t, a_t) =\\sigma_t^V\\\\\n",
    "&A^2_\\theta(s_t, a_t) = \\sigma_t^V + \\gamma \\sigma_{t+1}^V\\\\\n",
    "&A^3_\\theta(s_t, a_t) = \\sigma_t^V + \\gamma \\sigma_{t+1}^V + \\gamma^2\\sigma_{t+2}^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aefa63-23d6-431f-85a3-7067371578aa",
   "metadata": {},
   "source": [
    "## 3.Generalized Advantage Estimation(GAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77217ae8-174a-4e18-b7aa-955cb787593e",
   "metadata": {},
   "source": [
    "$$A_\\theta^{GAE}(s_t, a_t) = (1-\\lambda)(A_\\theta^1+ \\lambda A_\\theta^2+ \\lambda^2A_\\theta^3+....) = \\sum_{b=0}^\\infty(\\gamma \\lambda)^b\\sigma_{t+b}^V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fef29d-54f9-4ae2-8ac1-b90f05ae3a9a",
   "metadata": {},
   "source": [
    "$$\\text{replacement:  } J(\\theta)=\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}A_\\theta^{GAE}(s_t, a_t)\\nabla \\log P_\\theta(a_n^t \\mid s_n^t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87478d5c-29c5-4e37-90cd-508ea9269b99",
   "metadata": {},
   "source": [
    "## 4.Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896341dc-c7a3-4acb-a420-b0c4e8b7a349",
   "metadata": {},
   "source": [
    "**Off Policy**: Use one model for collecting trajectories and use those data to train other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b4d9c4-44e9-41c3-a8ce-7a286a68edf9",
   "metadata": {},
   "source": [
    "**Importance sampling**:\n",
    "\\begin{align*}\n",
    "E(f(x))_{x \\sim p(x)} = &\\sum_{x}f(x)*p(x)\\\\\n",
    "                      = &\\sum_{x}f(x)*p(x)\\frac{q(x)}{q(x)}\\\\\n",
    "                      = &\\sum_{x}f(x)\\frac{p(x)}{q(x)}*q(x)\\\\\n",
    "                      = &E(f(x)\\frac{p(x)}{q(x)})_{x\\sim q(x)}\\\\\n",
    "                      = &\\frac{1}{N}\\sum_{n=1}^Nf(x)\\frac{p(x)}{q(x)}_{x\\sim q(x)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad424461-3ba5-421f-9c4f-efce0b29a377",
   "metadata": {},
   "source": [
    "$$\\text{replacement:  } J(\\theta)=\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}A_{\\theta'}^{GAE}(s_t, a_t)\\frac{P_\\theta(a_n^t\\mid s_n^t)}{P_{\\theta'}(a_n^t\\mid s_n^t)}\\nabla \\log P_\\theta(a_n^t \\mid s_n^t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46724652-96c7-47f5-83e1-8a00c61c2fc6",
   "metadata": {},
   "source": [
    "$$\\text{replacement:  } J(\\theta)=\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}A_\\theta'^{GAE}(s_t, a_t)\\frac{\\nabla P_\\theta(a_n^t\\mid s_n^t)}{P_\\theta'(a_n^t\\mid s_n^t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea170e62-0ee3-4aa6-888a-4cbf7b40450a",
   "metadata": {},
   "source": [
    "$$Loss = -\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}A_{\\theta'}^{GAE}(s_t, a_t)\\frac{\\nabla P_\\theta(a_n^t\\mid s_n^t)}{P_{\\theta'}(a_n^t\\mid s_n^t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ed5ec-15e6-4ec8-b79c-47d0eab6ac7e",
   "metadata": {},
   "source": [
    "$$ \\text{Now we can sample with policy } \\theta' \\text{and updata our policy }\\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1061d5c-83ff-4c9d-a1df-34fb35960c5d",
   "metadata": {},
   "source": [
    "$$\\text{Intuition for the ratio:\n",
    "Bigger probability to take the action means making bigger change}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a47b67-f188-4e44-8f3e-f457abb376f3",
   "metadata": {},
   "source": [
    "**Problem 3**:\\\n",
    "If the two policies have too distinctive distribution then the information can be useless.\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594ccc0-df66-42d3-9217-865487fb573f",
   "metadata": {},
   "source": [
    "Solution1:\\\n",
    "We use **kl-divergence** to evaluate their differences. (kl=0 if the two distributions are the same)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c676cfb-d921-40ab-b11c-b52010ad312f",
   "metadata": {},
   "source": [
    "$$Loss = -\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}A_{\\theta'}^{GAE}(s_t, a_t)\\frac{\\nabla P_\\theta(a_n^t\\mid s_n^t)}{P_{\\theta'}(a_n^t\\mid s_n^t)} + \\beta KL(P_\\theta,P_{\\theta'}) \\text{ (  }\\beta\\text{  is used for managing constraint}\\text{)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf208276-39ae-4b54-8587-044eba46871c",
   "metadata": {},
   "source": [
    "Solution2:\\\n",
    "We use clip method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b8f874-441c-4101-b757-06b17e3114be",
   "metadata": {},
   "source": [
    "$$Loss = -\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n}min(A_{\\theta'}^{GAE}(s_t, a_t)\\frac{ P_\\theta(a_n^t\\mid s_n^t)}{P_{\\theta'}(a_n^t\\mid s_n^t)},clip(\\frac{P_\\theta(a_n^t\\mid s_n^t)}{P_{\\theta'}(a_n^t\\mid s_n^t)},1-\\epsilon,1+\\epsilon)A_{\\theta'}^{GAE}(s_n^t,a_n^t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01718be-9d08-479e-a433-4d8fd69920d7",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5cf8e02-9adc-4405-a583-6eeac93ca128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import numpy as np\n",
    "import gymnasium as gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05305d4a-2b1b-44c8-bd86-12d76c9840b8",
   "metadata": {},
   "source": [
    "Write the rollout buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0fcf867-206a-4ee0-a275-78bbb2919f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "\n",
    "    def generate_batches(self,batch_size):\n",
    "        batches = []\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0,n_states,batch_size)\n",
    "        # indices = np.arange(n_states, dtype=np.int64)\n",
    "        #np.random.shuffle(indices) #diorder the indices for the sake of stochastic gradient descent\n",
    "        for i in batch_start:\n",
    "            c_i = i\n",
    "            t_batch = []\n",
    "            for j in range(batch_size):\n",
    "                t_batch.append(self.states[c_i])\n",
    "                c_i+=1\n",
    "            batches.append(t_batch)\n",
    "        return batches\n",
    "\n",
    "    def store(self, state, action, reward, value, log_prob, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.log_prob.append(log_prob)\n",
    "        self.dons.append(done)\n",
    "\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9685dfc6-a0d0-4144-b3bb-eafd47b2a399",
   "metadata": {},
   "source": [
    "Build Actor-Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33355b82-d4f6-42be-8f72-5d28006e699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, n_states, lr):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = 32\n",
    "        self.fc2 = 32\n",
    "        self.lr = lr\n",
    "        self.actor = nn.Sequential(nn.Linear(n_states, self.fc1),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(self.fc1, self.fc2),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(self.fc2, n_actions),\n",
    "                                  nn.Softmax(dim=-1))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        act_dis = self.actor(states)\n",
    "        act_dis = Categorical(act_dist)\n",
    "        return act_dis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80600e3c-64a8-4c37-a56b-1dbe069cfc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, n_states, lr):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = 32\n",
    "        self.fc2 = 32\n",
    "        self.lr = lr\n",
    "        self.critic = nn.Sequential(nn.Linear(n_states, self.fc1),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(self.fc1, self.fc2),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(self.fc2, 1))\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = self.lr)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, states):\n",
    "        value = self.critic(states)\n",
    "        return value        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cff94-271e-4aaf-a653-7931ed37c9b2",
   "metadata": {},
   "source": [
    "Implement the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8abc33a4-a96d-4053-851e-c95b21ef5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, n_actions, gamma=0.99, lr=0.0003, policy_clip=0.1, batch_size=32, N=2048, n_epochs=10, gae_lambda=0.95):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions, n_states, lr)\n",
    "        self.critic = CriticNetwork(n_states, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85eb3c3-865e-4c96-bb47-b0e376ff050b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
